syntax = "proto3";
package inference;

service GRPCInferenceService
{
    rpc ServerLive(ServerLiveRequest) returns (ServerLiveResponse) {}
    rpc ServerReady(ServerReadyRequest) returns (ServerReadyResponse) {}
    rpc ModelReady(ModelReadyRequest) returns (ModelReadyResponse) {}
    rpc ServerMetadata(ServerMetadataRequest) returns (ServerMetadataResponse) {}
    rpc ModelMetadata(ModelMetadataRequest) returns (ModelMetadataResponse) {}
    rpc ModelInfer(ModelInferRequest) returns (ModelInferResponse) {}
    rpc ModelLoad(ModelLoadRequest) returns (ModelLoadResponse) {}
    rpc ModelUnload(ModelUnloadRequest) returns (ModelUnloadResponse) {}
    rpc WorkerLoad(WorkerLoadRequest) returns (WorkerLoadResponse) {}
    rpc WorkerUnload(WorkerUnloadRequest) returns (WorkerUnloadResponse) {}
    rpc ModelList(ModelListRequest) returns (ModelListResponse) {}
    rpc HasHardware(HasHardwareRequest) returns (HasHardwareResponse) {}
}

message ServerLiveRequest {}
message ServerLiveResponse
{
    bool live = 1;
}
message ServerReadyRequest {}
message ServerReadyResponse
{
    bool ready = 1;
}
message ModelReadyRequest
{
    string name = 1;
    string version = 2;
}
message ModelReadyResponse
{
    bool ready = 1;
}
message ModelListRequest {}
message ModelListResponse
{
    repeated string models = 1;
}
message ServerMetadataRequest {}
message ServerMetadataResponse
{
    string name = 1;
    string version = 2;
    repeated string extensions = 3;
}
message ModelMetadataRequest
{
    string name = 1;
    string version = 2;
}
message ModelMetadataResponse
{
    message TensorMetadata
    {
        string name = 1;
        string datatype = 2;
        repeated int64 shape = 3;
    }
    string name = 1;
    repeated string versions = 2;
    string platform = 3;
    repeated TensorMetadata inputs = 4;
    repeated TensorMetadata outputs = 5;
}
message InferParameter
{
    oneof parameter_choice
    {
        bool bool_param = 1;
        int64 int64_param = 2;
        string string_param = 3;
        double double_param = 4;
    }
}
message InferTensorContents
{
    repeated bool bool_contents = 1;
    repeated int32 int_contents = 2;
    repeated int64 int64_contents = 3;
    repeated uint32 uint_contents = 4;
    repeated uint64 uint64_contents = 5;
    repeated float fp32_contents = 6;
    repeated double fp64_contents = 7;
    repeated bytes bytes_contents = 8;
}
message ModelInferRequest
{
    message InferInputTensor
    {
        string name = 1;
        string datatype = 2;
        repeated int64 shape = 3;
        map<string, InferParameter> parameters = 4;
        InferTensorContents contents = 5;
    }
    message InferRequestedOutputTensor
    {
        string name = 1;
        map<string, InferParameter> parameters = 2;
    }
    string model_name = 1;
    string model_version = 2;
    string id = 3;
    map<string, InferParameter> parameters = 4;
    repeated InferInputTensor inputs = 5;
    repeated InferRequestedOutputTensor outputs = 6;
    repeated bytes raw_input_contents = 7;
}
message ModelInferResponse
{
    message InferOutputTensor
    {
        string name = 1;
        string datatype = 2;
        repeated int64 shape = 3;
        map<string, InferParameter> parameters = 4;
        InferTensorContents contents = 5;
    }
    string model_name = 1;
    string model_version = 2;
    string id = 3;
    map<string, InferParameter> parameters = 4;
    repeated InferOutputTensor outputs = 5;
    repeated bytes raw_output_contents = 6;
}
message ModelLoadRequest
{
    string name = 1;
    map<string, InferParameter> parameters = 2;
}
message ModelLoadResponse {}
message ModelUnloadRequest
{
    string name = 1;
}
message ModelUnloadResponse {}
message WorkerLoadRequest
{
    string name = 1;
    map<string, InferParameter> parameters = 2;
}
message WorkerLoadResponse
{
    string endpoint = 1;
}
message WorkerUnloadRequest
{
    string name = 1;
}
message WorkerUnloadResponse {}
message HasHardwareRequest
{
    string name = 1;
    uint32 num = 2;
}
message HasHardwareResponse
{
    bool found = 1;
}
